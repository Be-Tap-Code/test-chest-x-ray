{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11279323,"sourceType":"datasetVersion","datasetId":7051766},{"sourceId":11279395,"sourceType":"datasetVersion","datasetId":7051820},{"sourceId":11279752,"sourceType":"datasetVersion","datasetId":7052047}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport cv2\nimport pickle\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, LSTM, Input, Embedding, Conv2D,Concatenate,Flatten,Add,Dropout,GRU\nimport random\nimport datetime\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom math import log\nimport torch\nfrom torchvision import transforms\nfrom transformers import ViTModel, ViTFeatureExtractor\nimport os\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:36:52.799224Z","iopub.execute_input":"2025-04-22T10:36:52.799531Z","iopub.status.idle":"2025-04-22T10:36:52.888378Z","shell.execute_reply.started":"2025-04-22T10:36:52.799509Z","shell.execute_reply":"2025-04-22T10:36:52.887631Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_dataset = pd.read_csv('/kaggle/input/data-split-csv/Train_Data.csv')\ncv_dataset = pd.read_csv('/kaggle/input/data-split-csv/CV_Data.csv')\ntest_dataset = pd.read_csv('/kaggle/input/data-split-csv/Test_Data.csv')\nX_train = train_dataset['Person_id']\ny_train = train_dataset['Report']\n\nX_cv = cv_dataset['Person_id']\ny_cv = cv_dataset['Report']\n\nX_test = test_dataset['Person_id']\ny_test = test_dataset['Report']\n\nmax_capt_len = 153\npad_size = max_capt_len ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:36:55.614388Z","iopub.execute_input":"2025-04-22T10:36:55.614860Z","iopub.status.idle":"2025-04-22T10:36:55.682132Z","shell.execute_reply.started":"2025-04-22T10:36:55.614835Z","shell.execute_reply":"2025-04-22T10:36:55.681597Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport re\nimport cv2\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nsns.set_style(\"whitegrid\")\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nwarnings.filterwarnings('ignore')\nimport xml.etree.ElementTree as ET\nfrom wordcloud import WordCloud, ImageColorGenerator\ncwd = os.getcwd()\nimport tarfile\nimages = tarfile.open('/kaggle/input/images-reports/NLMCXR_png.tgz')\nimages.extractall(cwd+'/xray_images/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:48:14.883493Z","iopub.execute_input":"2025-04-22T10:48:14.884067Z","iopub.status.idle":"2025-04-22T10:48:37.572126Z","shell.execute_reply.started":"2025-04-22T10:48:14.884046Z","shell.execute_reply":"2025-04-22T10:48:37.571362Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:36:57.362503Z","iopub.execute_input":"2025-04-22T10:36:57.362782Z","iopub.status.idle":"2025-04-22T10:36:57.384829Z","shell.execute_reply.started":"2025-04-22T10:36:57.362761Z","shell.execute_reply":"2025-04-22T10:36:57.384131Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                             Person_id  \\\n0      Scanned Images/CXR443_IM-2078_0   \n1     Scanned Images/CXR2552_IM-1058_0   \n2     Scanned Images/CXR1622_IM-0404_0   \n3     Scanned Images/CXR3986_IM-2041_0   \n4     Scanned Images/CXR3986_IM-2041_1   \n...                                ...   \n2745  Scanned Images/CXR3102_IM-1454_0   \n2746  Scanned Images/CXR1634_IM-0414_0   \n2747  Scanned Images/CXR3849_IM-1947_0   \n2748    Scanned Images/CXR1606_IM-0394   \n2749  Scanned Images/CXR1550_IM-0359_0   \n\n                                       Image1  \\\n0      Scanned Images/CXR443_IM-2078-1001.png   \n1     Scanned Images/CXR2552_IM-1058-1001.png   \n2     Scanned Images/CXR1622_IM-0404-1001.png   \n3     Scanned Images/CXR3986_IM-2041-1001.png   \n4     Scanned Images/CXR3986_IM-2041-1001.png   \n...                                       ...   \n2745  Scanned Images/CXR3102_IM-1454-1001.png   \n2746  Scanned Images/CXR1634_IM-0414-2002.png   \n2747  Scanned Images/CXR3849_IM-1947-1001.png   \n2748  Scanned Images/CXR1606_IM-0394-2001.png   \n2749  Scanned Images/CXR1550_IM-0359-1001.png   \n\n                                       Image2  \\\n0      Scanned Images/CXR443_IM-2078-1002.png   \n1     Scanned Images/CXR2552_IM-1058-1002.png   \n2     Scanned Images/CXR1622_IM-0404-2001.png   \n3     Scanned Images/CXR3986_IM-2041-2001.png   \n4     Scanned Images/CXR3986_IM-2041-3001.png   \n...                                       ...   \n2745  Scanned Images/CXR3102_IM-1454-2001.png   \n2746  Scanned Images/CXR1634_IM-0414-3003.png   \n2747  Scanned Images/CXR3849_IM-1947-2001.png   \n2748  Scanned Images/CXR1606_IM-0394-2001.png   \n2749  Scanned Images/CXR1550_IM-0359-3003.png   \n\n                                                 Report  \n0     startseq heart size and mediastinal contour wi...  \n1     startseq lungs are clear without focal consoli...  \n2     startseq the cardiomediastinal silhouette with...  \n3     startseq the cardiomediastinal silhouette and ...  \n4     startseq the cardiomediastinal silhouette and ...  \n...                                                 ...  \n2745  startseq the lungs are clear bilaterally .  sp...  \n2746  startseq calcified granulomas .  calcified hil...  \n2747  startseq cardiomediastinal silhouettes are wit...  \n2748  startseq the heart normal size .  the mediasti...  \n2749  startseq heart size cardiomediastinal silhouet...  \n\n[2750 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Person_id</th>\n      <th>Image1</th>\n      <th>Image2</th>\n      <th>Report</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Scanned Images/CXR443_IM-2078_0</td>\n      <td>Scanned Images/CXR443_IM-2078-1001.png</td>\n      <td>Scanned Images/CXR443_IM-2078-1002.png</td>\n      <td>startseq heart size and mediastinal contour wi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Scanned Images/CXR2552_IM-1058_0</td>\n      <td>Scanned Images/CXR2552_IM-1058-1001.png</td>\n      <td>Scanned Images/CXR2552_IM-1058-1002.png</td>\n      <td>startseq lungs are clear without focal consoli...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Scanned Images/CXR1622_IM-0404_0</td>\n      <td>Scanned Images/CXR1622_IM-0404-1001.png</td>\n      <td>Scanned Images/CXR1622_IM-0404-2001.png</td>\n      <td>startseq the cardiomediastinal silhouette with...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Scanned Images/CXR3986_IM-2041_0</td>\n      <td>Scanned Images/CXR3986_IM-2041-1001.png</td>\n      <td>Scanned Images/CXR3986_IM-2041-2001.png</td>\n      <td>startseq the cardiomediastinal silhouette and ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Scanned Images/CXR3986_IM-2041_1</td>\n      <td>Scanned Images/CXR3986_IM-2041-1001.png</td>\n      <td>Scanned Images/CXR3986_IM-2041-3001.png</td>\n      <td>startseq the cardiomediastinal silhouette and ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2745</th>\n      <td>Scanned Images/CXR3102_IM-1454_0</td>\n      <td>Scanned Images/CXR3102_IM-1454-1001.png</td>\n      <td>Scanned Images/CXR3102_IM-1454-2001.png</td>\n      <td>startseq the lungs are clear bilaterally .  sp...</td>\n    </tr>\n    <tr>\n      <th>2746</th>\n      <td>Scanned Images/CXR1634_IM-0414_0</td>\n      <td>Scanned Images/CXR1634_IM-0414-2002.png</td>\n      <td>Scanned Images/CXR1634_IM-0414-3003.png</td>\n      <td>startseq calcified granulomas .  calcified hil...</td>\n    </tr>\n    <tr>\n      <th>2747</th>\n      <td>Scanned Images/CXR3849_IM-1947_0</td>\n      <td>Scanned Images/CXR3849_IM-1947-1001.png</td>\n      <td>Scanned Images/CXR3849_IM-1947-2001.png</td>\n      <td>startseq cardiomediastinal silhouettes are wit...</td>\n    </tr>\n    <tr>\n      <th>2748</th>\n      <td>Scanned Images/CXR1606_IM-0394</td>\n      <td>Scanned Images/CXR1606_IM-0394-2001.png</td>\n      <td>Scanned Images/CXR1606_IM-0394-2001.png</td>\n      <td>startseq the heart normal size .  the mediasti...</td>\n    </tr>\n    <tr>\n      <th>2749</th>\n      <td>Scanned Images/CXR1550_IM-0359_0</td>\n      <td>Scanned Images/CXR1550_IM-0359-1001.png</td>\n      <td>Scanned Images/CXR1550_IM-0359-3003.png</td>\n      <td>startseq heart size cardiomediastinal silhouet...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2750 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Print first few reports to check the data\nprint(\"First 5 reports:\")\nfor i in range(5):\n    print(f\"\\nReport {i+1}:\")\n    print(train_dataset['Report'][i])\n    # Split report into words and get unique word count\n    words = set(str(train_dataset['Report'][i]).split())\n    print(f\"Unique words: {len(words)}\")\n\n# Calculate max vocabulary size per report\nmax_len = max([len(set(str(report).split())) for report in train_dataset['Report']])\nprint(f\"\\nMaximum vocabulary size in a single report: {max_len}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:37:01.602321Z","iopub.execute_input":"2025-04-22T10:37:01.602952Z","iopub.status.idle":"2025-04-22T10:37:01.621730Z","shell.execute_reply.started":"2025-04-22T10:37:01.602917Z","shell.execute_reply":"2025-04-22T10:37:01.620686Z"}},"outputs":[{"name":"stdout","text":"First 5 reports:\n\nReport 1:\nstartseq heart size and mediastinal contour within normal limits .  no focal airspace consolidation pneumothora large pleural effusion .  degenerative changes thoracic spine .  endseq\nUnique words: 23\n\nReport 2:\nstartseq lungs are clear without focal consolidation effusion pneumothora .  normal heart size .  bony thora and soft tissues grossly unremarkable endseq\nUnique words: 21\n\nReport 3:\nstartseq the cardiomediastinal silhouette within normal limits for size and contour .  the lungs are normally inflated without evidence focal airspace disease pleural effusion pneumothora .  no acute osseus abnormality .  endseq\nUnique words: 29\n\nReport 4:\nstartseq the cardiomediastinal silhouette and vasculature are within normal limits for size and contour .  the lungs are normally inflated and clear .  osseous structures are within normal limits for patient age .  endseq\nUnique words: 23\n\nReport 5:\nstartseq the cardiomediastinal silhouette and vasculature are within normal limits for size and contour .  the lungs are normally inflated and clear .  osseous structures are within normal limits for patient age .  endseq\nUnique words: 23\n\nMaximum vocabulary size in a single report: 80\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn # Need nn for Sequential\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport cv2\n\n# --- Import ResNet and Transforms (already done in Cell 1) ---\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\n# --- Setup Device ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- Load Pre-trained ResNet-152 ---\nprint(\"Loading pre-trained ResNet-152...\")\n# Load the pre-trained model with weights appropriate for inference\n# Using the updated API for weights is recommended\nresnet_model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n\n# Remove the final fully connected layer (classification layer)\n# Features are extracted from the layer before it (avgpool)\n# Access layers by name or structure (safer with names if possible, but structure works)\nmodules = list(resnet_model.children())[:-1] # Exclude the last layer (fc)\nresnet_feature_extractor = nn.Sequential(*modules)\n\n# Set the model to evaluation mode and move to device\nresnet_feature_extractor.eval()\nresnet_feature_extractor = resnet_feature_extractor.to(device)\nprint(\"ResNet-152 feature extractor loaded and moved to device.\")\n\n# --- Define ImageNet Preprocessing Transforms ---\n# Standard transforms for ImageNet models\n# Ensure input images are resized/cropped to 224x224\npreprocess = transforms.Compose([\n    transforms.Resize(256),             # Resize smaller side to 256\n    transforms.CenterCrop(224),         # Crop center 224x224\n    transforms.ToTensor(),              # Convert image to PyTorch Tensor (scales to [0, 1])\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize\n])\n\n# --- Updated Image Loading and Feature Extraction Functions ---\n\ndef load_and_preprocess_image(img_path):\n    \"\"\"Loads an image, ensures 3 channels (RGB), and applies ResNet preprocessing.\"\"\"\n    try:\n        img = Image.open(img_path).convert('RGB') # Ensure image is RGB\n        img_tensor = preprocess(img)\n        # Add batch dimension (unsqueeze(0)) -> Shape: [1, 3, 224, 224]\n        return img_tensor.unsqueeze(0)\n    except FileNotFoundError:\n        print(f\"⚠️ File not found: {img_path}\")\n        return None\n    except Exception as e:\n        print(f\"⚠️ Error processing image {img_path}: {e}\")\n        return None\n\ndef extract_img_feature(image1_rel_path, image2_rel_path, base_path=\"/kaggle/working/xray_images\"):\n    \"\"\"Extracts and concatenates ResNet-152 features for two images.\"\"\"\n\n    # Helper function to extract features for a single image path (relative path)\n    def extract_single_feature(relative_img_path):\n        # Construct full path using the base path and relative path\n        filename = os.path.basename(relative_img_path.strip())\n        full_path = os.path.join(base_path, filename)\n\n        # Load and preprocess the image\n        image_tensor = load_and_preprocess_image(full_path)\n        if image_tensor is None:\n            return None # Return None if image loading/preprocessing failed\n\n        # Move tensor to the correct device (GPU or CPU)\n        image_tensor = image_tensor.to(device)\n\n        # Extract features using ResNet feature extractor\n        with torch.no_grad(): # Disable gradient calculation for inference\n            features = resnet_feature_extractor(image_tensor)\n            # Output shape after nn.Sequential(*list(resnet_model.children())[:-1])\n            # is typically [batch_size, num_features, 1, 1] (e.g., [1, 2048, 1, 1])\n            # We need to flatten it to get a feature vector\n            features_flat = torch.flatten(features, 1) # Shape: [1, 2048]\n\n        # Squeeze unnecessary dimensions and move features to CPU as a NumPy array\n        return features_flat.squeeze().cpu().numpy() # Return as NumPy array (shape: (2048,))\n\n    # Extract features for both images using the helper function\n    img1_features = extract_single_feature(image1_rel_path)\n    img2_features = extract_single_feature(image2_rel_path)\n\n    # Concatenate features if both were extracted successfully\n    if img1_features is not None and img2_features is not None:\n        # Concatenate the 1D NumPy arrays\n        combined_feature = np.concatenate([img1_features, img2_features], axis=0)\n        # Resulting shape: (4096,)\n        return combined_feature\n    else:\n        # Handle cases where one or both images failed to process\n        print(f\"❌ Could not extract features for one or both images: {image1_rel_path}, {image2_rel_path}\")\n        # Return None to indicate failure for this pair\n        return None\n\ndef extract_features_for_datasets(image_df, base_path):\n    \"\"\"Extracts features for all image pairs in a DataFrame, handles failures.\"\"\"\n    features_list = []\n    successful_indices = [] # Keep track of original DataFrame indices\n\n    # Iterate through DataFrame rows with a progress bar\n    print(f\"\\nStarting feature extraction for {len(image_df)} pairs...\")\n    for index, row in tqdm(image_df.iterrows(), total=image_df.shape[0], desc=\"Extracting ResNet features\"):\n        # Get relative image paths from the DataFrame\n        img1_rel_path = row['Image1']\n        img2_rel_path = row['Image2']\n\n        # Extract combined features for the image pair\n        feature = extract_img_feature(img1_rel_path, img2_rel_path, base_path)\n\n        # Only append features and index if extraction was successful\n        if feature is not None:\n            features_list.append(feature)\n            successful_indices.append(index) # Store the original index\n\n    # Convert the list of successful features into a single NumPy array\n    features_array = np.array(features_list)\n    print(f\"\\nSuccessfully extracted features for {len(successful_indices)} out of {len(image_df)} image pairs.\")\n\n    # Return the feature array and the list of successful original indices\n    return features_array, successful_indices\n\n# --- Prepare DataFrames (assuming train_dataset, cv_dataset, test_dataset loaded in Cell 2) ---\nprint(\"\\nPreparing DataFrames...\")\n# Select necessary columns, keeping the original index\ntrain_images_df = train_dataset[['Image1', 'Image2', 'Report']].copy()\ncv_images_df = cv_dataset[['Image1', 'Image2', 'Report']].copy()\ntest_images_df = test_dataset[['Image1', 'Image2', 'Report']].copy()\nprint(\"DataFrames prepared.\")\n\n# --- Run Feature Extraction ---\n# Define the base path where images were extracted (e.g., by tarfile in Cell 3)\nbase_image_path = '/kaggle/working/xray_images' # Verify this path is correct\nprint(f\"Base image path set to: {base_image_path}\")\n\n# Extract features for training, validation, and test sets\nX_train_features, train_indices = extract_features_for_datasets(train_images_df, base_image_path)\nX_cv_features, cv_indices = extract_features_for_datasets(cv_images_df, base_image_path)\nX_test_features, test_indices = extract_features_for_datasets(test_images_df, base_image_path)\n\n# --- Filter Reports based on Successful Feature Extraction ---\n# Ensure the reports correspond exactly to the successfully extracted features\n# Use the stored original indices (.loc) to select the correct reports\nprint(\"\\nFiltering reports to match successful feature extractions...\")\ny_train_filtered = train_images_df.loc[train_indices, 'Report'].tolist()\ny_cv_filtered = cv_images_df.loc[cv_indices, 'Report'].tolist()\ny_test_filtered = test_images_df.loc[test_indices, 'Report'].tolist()\nprint(\"Reports filtered.\")\n\n# --- Reassign variables for consistency with subsequent cells ---\n# Overwrite old variables with the filtered data\nX_train = X_train_features\nX_cv = X_cv_features\nX_test = X_test_features\n# Overwrite report lists with the filtered versions\ny_train = y_train_filtered\ny_cv = y_cv_filtered\ny_test = y_test_filtered\n\n# Print shapes of the final feature arrays and the number of corresponding reports\nprint(\"\\n--- Final Data Shapes After Filtering ---\")\nprint(f\"X_train shape: {X_train.shape}, y_train count: {len(y_train)}\")\nprint(f\"X_cv shape: {X_cv.shape},    y_cv count: {len(y_cv)}\")\nprint(f\"X_test shape: {X_test.shape},  y_test count: {len(y_test)}\")\n\n# Sanity check: Number of samples should match between features and reports\nassert X_train.shape[0] == len(y_train), \"Mismatch between X_train features and y_train reports!\"\nassert X_cv.shape[0] == len(y_cv), \"Mismatch between X_cv features and y_cv reports!\"\nassert X_test.shape[0] == len(y_test), \"Mismatch between X_test features and y_test reports!\"\nprint(\"Data shapes verified.\")\n\nprint(\"\\nResNet-152 feature extraction complete.\")\n\n# --- Optional: Clear ResNet model from memory if needed ---\n# Uncomment the following lines if GPU memory is tight before loading BioBART\n# del resnet_model\n# del resnet_feature_extractor\n# if device == torch.device(\"cuda\"):\n#     torch.cuda.empty_cache()\n# print(\"ResNet model cleared from memory (optional).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:48:40.487548Z","iopub.execute_input":"2025-04-22T10:48:40.488406Z","iopub.status.idle":"2025-04-22T10:51:33.764118Z","shell.execute_reply.started":"2025-04-22T10:48:40.488379Z","shell.execute_reply":"2025-04-22T10:51:33.763427Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading pre-trained ResNet-152...\nResNet-152 feature extractor loaded and moved to device.\n\nPreparing DataFrames...\nDataFrames prepared.\nBase image path set to: /kaggle/working/xray_images\n\nStarting feature extraction for 2750 pairs...\n","output_type":"stream"},{"name":"stderr","text":"Extracting ResNet features: 100%|██████████| 2750/2750 [02:08<00:00, 21.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nSuccessfully extracted features for 2750 out of 2750 image pairs.\n\nStarting feature extraction for 563 pairs...\n","output_type":"stream"},{"name":"stderr","text":"Extracting ResNet features: 100%|██████████| 563/563 [00:25<00:00, 21.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nSuccessfully extracted features for 563 out of 563 image pairs.\n\nStarting feature extraction for 394 pairs...\n","output_type":"stream"},{"name":"stderr","text":"Extracting ResNet features: 100%|██████████| 394/394 [00:18<00:00, 21.56it/s]","output_type":"stream"},{"name":"stdout","text":"\nSuccessfully extracted features for 394 out of 394 image pairs.\n\nFiltering reports to match successful feature extractions...\nReports filtered.\n\n--- Final Data Shapes After Filtering ---\nX_train shape: (2750, 4096), y_train count: 2750\nX_cv shape: (563, 4096),    y_cv count: 563\nX_test shape: (394, 4096),  y_test count: 394\nData shapes verified.\n\nResNet-152 feature extraction complete.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n# Removed GPT-2 imports, BART imports are in Cell 1\nfrom transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\nfrom tqdm import tqdm # Keep tqdm if used here\nimport re # Keep re if used here\n\nclass MLP(nn.Module):\n    \"\"\" MLP to map concatenated ResNet features to BioBART's hidden size.\"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(MLP, self).__init__()\n        # Simple MLP: Linear -> ReLU -> Dropout -> Linear -> ReLU -> Dropout -> Linear -> LayerNorm\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.dropout1 = nn.Dropout(0.2) # Increased dropout slightly\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.dropout2 = nn.Dropout(0.2) # Increased dropout slightly\n        self.fc3 = nn.Linear(hidden_dim, output_dim)\n        self.layer_norm = nn.LayerNorm(output_dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = torch.relu(x)\n        x = self.dropout2(x)\n        x = self.fc3(x)\n        x = self.layer_norm(x)\n        return x\n\n# --- Load BioBART Tokenizer and Model (Using BioBART Base) ---\n\nbiobart_model_name = \"GanjinZero/biobart-base\" # Or GanjinZero/biobart-large\nprint(f\"Loading BioBART tokenizer: {biobart_model_name}\")\n# Load tokenizer associated with the chosen BioBART model\nbiobart_tokenizer = BartTokenizer.from_pretrained(biobart_model_name)\n\nprint(f\"Loading BioBART model: {biobart_model_name}\")\n# Load the pre-trained BioBART model for sequence generation\nbiobart_model = BartForConditionalGeneration.from_pretrained(biobart_model_name)\n\n# --- Initialize MLP ---\n# Get the hidden size (d_model) from the loaded BioBART model's configuration\nmlp_output_dim = biobart_model.config.d_model\nprint(f\"BioBART hidden size (MLP output dim): {mlp_output_dim}\")\n\n# *** IMPORTANT: Update MLP input dimension for concatenated ResNet-152 features ***\n# ResNet-152 feature dimension = 2048\n# Concatenated feature dimension = 2048 * 2 = 4096\nmlp_input_dim = 4096\nmlp_hidden_dim = 1024 # Hidden dimension for the MLP (can be tuned)\nprint(f\"Initializing MLP: Input Dim={mlp_input_dim}, Hidden Dim={mlp_hidden_dim}, Output Dim={mlp_output_dim}\")\n\nmlp = MLP(input_dim=mlp_input_dim, hidden_dim=mlp_hidden_dim, output_dim=mlp_output_dim)\n\n# --- Setup Device (Ensure device is defined, usually from previous cells) ---\nif 'device' not in locals():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device defined in this cell: {device}\")\nelse:\n    print(f\"Using device defined previously: {device}\")\n\n# Move models to the selected device\nmlp.to(device)\nbiobart_model.to(device)\nprint(f\"MLP and BioBART models moved to device: {device}\")\n\n# --- Display Tokenizer Info (Optional but helpful) ---\nprint(f\"\\n--- Tokenizer Info ---\")\nprint(f\"BioBART Tokenizer Vocabulary Size: {biobart_tokenizer.vocab_size}\")\nprint(f\"Pad Token: '{biobart_tokenizer.pad_token}' (ID: {biobart_tokenizer.pad_token_id})\")\nprint(f\"BOS Token: '{biobart_tokenizer.bos_token}' (ID: {biobart_tokenizer.bos_token_id})\")\nprint(f\"EOS Token: '{biobart_tokenizer.eos_token}' (ID: {biobart_tokenizer.eos_token_id})\")\n# Check the decoder start token ID used by the model configuration\nprint(f\"Decoder Start Token ID: {biobart_model.config.decoder_start_token_id} (Should match EOS ID for standard BART)\")\n\nprint(\"\\nModel and MLP setup complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:53:35.911625Z","iopub.execute_input":"2025-04-22T10:53:35.911924Z","iopub.status.idle":"2025-04-22T10:53:36.850963Z","shell.execute_reply.started":"2025-04-22T10:53:35.911903Z","shell.execute_reply":"2025-04-22T10:53:36.850354Z"}},"outputs":[{"name":"stdout","text":"Loading BioBART tokenizer: GanjinZero/biobart-base\nLoading BioBART model: GanjinZero/biobart-base\nBioBART hidden size (MLP output dim): 768\nInitializing MLP: Input Dim=4096, Hidden Dim=1024, Output Dim=768\nUsing device defined previously: cuda\nMLP and BioBART models moved to device: cuda\n\n--- Tokenizer Info ---\nBioBART Tokenizer Vocabulary Size: 50265\nPad Token: '<pad>' (ID: 1)\nBOS Token: '<s>' (ID: 0)\nEOS Token: '</s>' (ID: 2)\nDecoder Start Token ID: 2 (Should match EOS ID for standard BART)\n\nModel and MLP setup complete.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"y_train[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:53:36.851926Z","iopub.execute_input":"2025-04-22T10:53:36.852236Z","iopub.status.idle":"2025-04-22T10:53:36.857061Z","shell.execute_reply.started":"2025-04-22T10:53:36.852220Z","shell.execute_reply":"2025-04-22T10:53:36.856380Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['startseq heart size and mediastinal contour within normal limits .  no focal airspace consolidation pneumothora large pleural effusion .  degenerative changes thoracic spine .  endseq',\n 'startseq lungs are clear without focal consolidation effusion pneumothora .  normal heart size .  bony thora and soft tissues grossly unremarkable endseq',\n 'startseq the cardiomediastinal silhouette within normal limits for size and contour .  the lungs are normally inflated without evidence focal airspace disease pleural effusion pneumothora .  no acute osseus abnormality .  endseq',\n 'startseq the cardiomediastinal silhouette and vasculature are within normal limits for size and contour .  the lungs are normally inflated and clear .  osseous structures are within normal limits for patient age .  endseq',\n 'startseq the cardiomediastinal silhouette and vasculature are within normal limits for size and contour .  the lungs are normally inflated and clear .  osseous structures are within normal limits for patient age .  endseq',\n 'startseq the cardiomediastinal silhouette and vasculature are within normal limits for size and contour .  the lungs are normally inflated and clear .  osseous structures are within normal limits for patient age .  endseq',\n 'startseq the heart and mediastinum are unremarkable .  the lung volumes are low .  there calcified granuloma the right hilum .  minimal atelectasis scarring the left lower lobe .  there no effusion pneumothora .  endseq',\n 'startseq the heart size and mediastinal contours appear within normal limits .  there are low lung volumes with left basilar subsegmental atelectasis .  no focal airspace consolidation effusions pneumothora .  no acute bony abnormalities .  endseq',\n 'startseq lungs are clear .  no pleural effusions pneumothoraces .  heart and mediastinum normal size and contour .  endseq',\n 'startseq mediastinal contours are normal .  lungs are clear .  there no pneumothora large pleural effusion .  endseq']"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Code from Cell 9 in the notebook file (should be Cell index 8)\nfrom torch.utils.data import TensorDataset, DataLoader\nimport re\nimport numpy as np # Ensure numpy is imported\n\n# Access mlp_input_dim defined in the previous cell (Cell 7)\nif 'mlp_input_dim' not in locals():\n     mlp_input_dim = 4096\n     print(f\"Warning: mlp_input_dim not found in locals. Defaulting to {mlp_input_dim}.\")\n\n# Clean text function (remains the same)\ndef clean_text(text):\n    # ... (implementation) ...\n    text = str(text); text = re.sub(r'[^\\w\\s.]', '', text); text = ' '.join(text.split()); return text\n\n# Clean and preprocess the reports\nprint(\"Cleaning reports...\")\ntry:\n    # Ensure y_train, y_cv, y_test are the filtered lists from Cell 6\n    if not isinstance(y_train, list) or not isinstance(y_cv, list) or not isinstance(y_test, list):\n         raise TypeError(\"y_train, y_cv, y_test should be lists (filtered reports from Cell 6).\")\n    y_train_cleaned = [clean_text(report) for report in y_train]\n    y_cv_cleaned = [clean_text(report) for report in y_cv]\n    y_test_cleaned = [clean_text(report) for report in y_test]\n    print(f\"Cleaned {len(y_train_cleaned)} train, {len(y_cv_cleaned)} CV, {len(y_test_cleaned)} test reports.\")\nexcept Exception as e:\n    print(f\"Error during report cleaning: {e}\")\n    y_train_cleaned, y_cv_cleaned, y_test_cleaned = [], [], []\n\n# Print sample reports after cleaning\n# ... (prints samples) ...\n\n# Normalize image features\nprint(\"\\nNormalizing image features...\")\ndef normalize_features(features_array, name):\n    # ... (implementation uses axis=0 mean/std, handles zero std) ...\n    if isinstance(features_array, np.ndarray) and features_array.size > 0:\n        mean, std = features_array.mean(axis=0), features_array.std(axis=0)\n        std = np.where(std < 1e-6, 1.0, std)\n        print(f\"Normalizing {name} features (shape: {features_array.shape})...\")\n        return (features_array - mean) / std\n    else:\n        print(f\"Warning: {name} features not found/empty. Skipping normalization.\")\n        return np.empty((0, mlp_input_dim))\n\ntry:\n    X_train_norm = normalize_features(X_train, 'X_train')\n    X_cv_norm = normalize_features(X_cv, 'X_cv')\n    X_test_norm = normalize_features(X_test, 'X_test')\nexcept NameError:\n     print(\"Error: X_train, X_cv, or X_test features not found (expected from Cell 6).\")\n     X_train_norm, X_cv_norm, X_test_norm = [np.empty((0, mlp_input_dim))]*3\n\n# Print statistics of normalized features\n# ... (prints stats) ...\n\n# Convert features to tensors\nprint(\"\\nConverting features to tensors...\")\ndef features_to_tensor(features_array, name):\n    # ... (implementation uses torch.tensor, .copy(), dtype=torch.float) ...\n     if isinstance(features_array, np.ndarray) and features_array.size > 0:\n         return torch.tensor(features_array.copy(), dtype=torch.float)\n     else:\n         print(f\"Warning: {name} features empty/invalid. Creating empty tensor.\")\n         return torch.empty((0, mlp_input_dim), dtype=torch.float)\n\nX_train_tensor = features_to_tensor(X_train_norm, 'X_train_norm')\nX_cv_tensor = features_to_tensor(X_cv_norm, 'X_cv_norm')\nX_test_tensor = features_to_tensor(X_test_norm, 'X_test_norm')\n\n# --- Tokenize reports using BioBART tokenizer ---\nprint(\"\\nTokenizing reports with BioBART tokenizer...\")\nmax_length = 150 # Sequence length for reports\ntry:\n    # ... (implementation uses biobart_tokenizer, handles empty lists) ...\n    y_train_tensor = biobart_tokenizer(y_train_cleaned, add_special_tokens=True, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')[\"input_ids\"] if y_train_cleaned else torch.empty((0, max_length)).long()\n    y_cv_tensor = biobart_tokenizer(y_cv_cleaned, add_special_tokens=True, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')[\"input_ids\"] if y_cv_cleaned else torch.empty((0, max_length)).long()\n    y_test_tensor = biobart_tokenizer(y_test_cleaned, add_special_tokens=True, truncation=True, max_length=max_length, padding='max_length', return_tensors='pt')[\"input_ids\"] if y_test_cleaned else torch.empty((0, max_length)).long()\n    print(\"Tokenization complete.\")\nexcept NameError:\n    print(\"Error: biobart_tokenizer not defined. Run Cell 7 first.\")\n    y_train_tensor, y_cv_tensor, y_test_tensor = [torch.empty((0, max_length)).long()]*3\n\n# --- Create datasets and dataloaders ---\nprint(\"\\nCreating Datasets and DataLoaders...\")\nbatch_size = 16\ndef create_dataloader(X_tensor, y_tensor, batch_size, shuffle=False, name=\"\"):\n    # ... (implementation handles empty tensors, checks shape match) ...\n    if X_tensor.shape[0] > 0 and y_tensor.shape[0] > 0:\n        if X_tensor.shape[0] == y_tensor.shape[0]:\n            dataset = TensorDataset(X_tensor, y_tensor)\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n            print(f\"  {name} DataLoader created with {len(dataset)} samples.\")\n            return dataset, dataloader\n        else:\n            print(f\"Error creating {name} DataLoader: Mismatch samples features ({X_tensor.shape[0]}) vs reports ({y_tensor.shape[0]}).\")\n            return None, None\n    else:\n        print(f\"Warning: {name} data tensors empty. Cannot create DataLoader.\")\n        return None, None\n\ntrain_dataset, train_loader = create_dataloader(X_train_tensor, y_train_tensor, batch_size, shuffle=True, name=\"Training\")\ncv_dataset, cv_loader = create_dataloader(X_cv_tensor, y_cv_tensor, batch_size, shuffle=False, name=\"Validation\")\ntest_dataset, test_loader = create_dataloader(X_test_tensor, y_test_tensor, batch_size, shuffle=False, name=\"Test\")\n\n# Print dataset information summary\n# ... (prints sizes) ...\n# Print sample tokenization check\n# ... (prints sample) ...\n# Check tensor shapes\n# ... (prints shapes) ...\n# Check first batch from dataloader\n# ... (prints batch shapes) ...\n\nprint(\"\\nPreprocessing complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:56:50.036525Z","iopub.execute_input":"2025-04-22T10:56:50.037247Z","iopub.status.idle":"2025-04-22T10:56:51.547175Z","shell.execute_reply.started":"2025-04-22T10:56:50.037224Z","shell.execute_reply":"2025-04-22T10:56:51.546445Z"}},"outputs":[{"name":"stdout","text":"Cleaning reports...\nCleaned 2750 train, 563 CV, 394 test reports.\n\nNormalizing image features...\nNormalizing X_train features (shape: (2750, 4096))...\nNormalizing X_cv features (shape: (563, 4096))...\nNormalizing X_test features (shape: (394, 4096))...\n\nConverting features to tensors...\n\nTokenizing reports with BioBART tokenizer...\nTokenization complete.\n\nCreating Datasets and DataLoaders...\n  Training DataLoader created with 2750 samples.\n  Validation DataLoader created with 563 samples.\n  Test DataLoader created with 394 samples.\n\nPreprocessing complete.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Code from Cell 11 in the notebook file (should be Cell index 10)\nfrom tqdm import tqdm\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport os # Import os for path checking\n\n# --- Redefine train_step and validate just in case they weren't run ---\n# (Code for train_step and validate functions is assumed to be correct from previous step)\ndef train_step(mlp, biobart_model, inputs, labels, optimizer, device):\n    # ... (Implementation: models.train(), move data, zero_grad, MLP forward, prepare decoder embeds, BART forward, loss.backward, clip_grad, optimizer.step) ...\n    mlp.train(); biobart_model.train()\n    inputs = inputs.to(device); labels = labels.to(device)\n    optimizer.zero_grad()\n    embedded_input = mlp(inputs)\n    batch_size, seq_len = labels.size()\n    decoder_inputs_embeds = embedded_input.unsqueeze(1).expand(-1, seq_len, -1)\n    decoder_attention_mask = (labels != biobart_tokenizer.pad_token_id).long().to(device)\n    outputs = biobart_model(inputs_embeds=decoder_inputs_embeds, labels=labels, decoder_attention_mask=decoder_attention_mask)\n    loss = outputs.loss\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(list(mlp.parameters()) + list(biobart_model.parameters()), max_norm=1.0)\n    optimizer.step()\n    return loss.item()\n\ndef validate(mlp, biobart_model, val_loader, device):\n    # ... (Implementation: models.eval(), no_grad, loop through loader, MLP forward, prepare decoder embeds, BART forward, accumulate loss) ...\n    mlp.eval(); biobart_model.eval()\n    total_val_loss = 0; num_batches = 0\n    with torch.no_grad():\n        for input_batch, label_batch in val_loader:\n            input_batch = input_batch.to(device); label_batch = label_batch.to(device)\n            embedded_input = mlp(input_batch)\n            batch_size, seq_len = label_batch.size()\n            decoder_inputs_embeds = embedded_input.unsqueeze(1).expand(-1, seq_len, -1)\n            decoder_attention_mask = (label_batch != biobart_tokenizer.pad_token_id).long().to(device)\n            outputs = biobart_model(inputs_embeds=decoder_inputs_embeds, labels=label_batch, decoder_attention_mask=decoder_attention_mask)\n            total_val_loss += outputs.loss.item(); num_batches += 1\n    return total_val_loss / num_batches if num_batches > 0 else float('inf')\n\n# --- Training Setup ---\n# Initialize optimizer\nlr_mlp = 1e-4\nlr_bart = 5e-5 # Fine-tuning LR for BioBART\noptimizer = optim.AdamW([\n    {'params': mlp.parameters(), 'lr': lr_mlp},\n    {'params': biobart_model.parameters(), 'lr': lr_bart}\n], weight_decay=0.01)\n\n# Scheduler\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\n# Training loop parameters\nepochs = 10\nbest_val_loss = float('inf')\npatience = 5\npatience_counter = 0\n# *** FIX: Update checkpoint path name ***\ncheckpoint_path = 'resnet152_mlp_biobart_best.pt' # Changed from vit_...\n\n# Check if dataloaders exist\nif 'train_loader' not in locals() or train_loader is None or \\\n   'cv_loader' not in locals() or cv_loader is None:\n    print(\"Error: Training or validation dataloader is missing. Cannot start training.\")\nelse:\n    print(f\"Starting training loop... Checkpoint path: {checkpoint_path}\")\n    if device == torch.device(\"cuda\"): torch.cuda.empty_cache()\n\n    for epoch in range(epochs):\n        total_train_loss = 0; num_train_batches = 0\n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n\n        # --- Training Phase ---\n        mlp.train(); biobart_model.train()\n        for batch_idx, (input_batch, label_batch) in enumerate(progress_bar):\n            try:\n                loss = train_step(mlp, biobart_model, input_batch, label_batch, optimizer, device)\n                total_train_loss += loss; num_train_batches += 1\n                progress_bar.set_postfix({'train_loss': f'{loss:.4f}'})\n\n                # --- Generate Sample Text Periodically ---\n                if batch_idx > 0 and batch_idx % 200 == 0: # Adjust frequency if needed\n                    # ... (Implementation uses models.eval(), generate with beam search, decode, print) ...\n                    mlp.eval(); biobart_model.eval()\n                    with torch.no_grad():\n                        sample_input = input_batch[:1].to(device)\n                        sample_embed = mlp(sample_input)\n                        sample_embed_gen = sample_embed.unsqueeze(1)\n                        gen_ids = biobart_model.generate(\n                            inputs_embeds=sample_embed_gen, max_length=100, num_beams=4,\n                            early_stopping=True, pad_token_id=biobart_tokenizer.pad_token_id,\n                            eos_token_id=biobart_tokenizer.eos_token_id\n                        )\n                        generated_text = biobart_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n                        print(f\"\\n--- Sample generation (Epoch {epoch+1}, Batch {batch_idx}) ---\")\n                        print(f\"{generated_text.strip()}\")\n                        print(\"----------------------------------------------------------\\n\")\n                    mlp.train(); biobart_model.train() # Return to train mode\n\n                # Clear cache periodically\n                if batch_idx > 0 and batch_idx % 50 == 0 and device == torch.device(\"cuda\"):\n                    torch.cuda.empty_cache()\n\n            except RuntimeError as e:\n                # ... (OOM handling) ...\n                if \"out of memory\" in str(e):\n                    print(f\"\\nWarning: CUDA OOM Error in batch {batch_idx}. Skipping.\"); torch.cuda.empty_cache(); optimizer.zero_grad(); continue\n                else: print(f\"Runtime Error in batch {batch_idx}: {str(e)}\"); raise e\n\n        # --- Validation Phase ---\n        avg_train_loss = total_train_loss / num_train_batches if num_train_batches > 0 else float('inf')\n        avg_val_loss = validate(mlp, biobart_model, cv_loader, device)\n        progress_bar.close()\n        print(f\"\\nEpoch {epoch + 1}/{epochs} Summary:\")\n        print(f\"  Average Training Loss: {avg_train_loss:.4f}\")\n        print(f\"  Validation Loss:       {avg_val_loss:.4f}\")\n\n        # --- Learning Rate Scheduling & Checkpointing ---\n        scheduler.step(avg_val_loss)\n        if avg_val_loss < best_val_loss:\n            print(f\"  Validation loss improved ({best_val_loss:.4f} --> {avg_val_loss:.4f}). Saving model...\")\n            best_val_loss = avg_val_loss; patience_counter = 0\n            # Save checkpoint\n            torch.save({\n                'epoch': epoch, 'mlp_state_dict': mlp.state_dict(), 'biobart_state_dict': biobart_model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(),\n                'train_loss': avg_train_loss, 'val_loss': avg_val_loss,\n                'biobart_model_name': biobart_model_name # Keep model name\n            }, checkpoint_path) # Use updated path name\n            print(f\"  Model saved to {checkpoint_path}\")\n        else:\n            patience_counter += 1\n            print(f\"  Validation loss did not improve. Patience: {patience_counter}/{patience}\")\n            if patience_counter >= patience:\n                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs.\")\n                break\n\n    print(\"\\nTraining finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T10:57:14.531638Z","iopub.execute_input":"2025-04-22T10:57:14.531889Z","iopub.status.idle":"2025-04-22T11:10:17.633168Z","shell.execute_reply.started":"2025-04-22T10:57:14.531872Z","shell.execute_reply":"2025-04-22T11:10:17.632370Z"}},"outputs":[{"name":"stdout","text":"Starting training loop... Checkpoint path: resnet152_mlp_biobart_best.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10 Summary:\n  Average Training Loss: 0.7607\n  Validation Loss:       0.4675\n  Validation loss improved (inf --> 0.4675). Saving model...\n  Model saved to resnet152_mlp_biobart_best.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10 Summary:\n  Average Training Loss: 0.4131\n  Validation Loss:       0.4200\n  Validation loss improved (0.4675 --> 0.4200). Saving model...\n  Model saved to resnet152_mlp_biobart_best.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/10 Summary:\n  Average Training Loss: 0.3568\n  Validation Loss:       0.3980\n  Validation loss improved (0.4200 --> 0.3980). Saving model...\n  Model saved to resnet152_mlp_biobart_best.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/10 Summary:\n  Average Training Loss: 0.3188\n  Validation Loss:       0.3901\n  Validation loss improved (0.3980 --> 0.3901). Saving model...\n  Model saved to resnet152_mlp_biobart_best.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/10 Summary:\n  Average Training Loss: 0.2891\n  Validation Loss:       0.3868\n  Validation loss improved (0.3901 --> 0.3868). Saving model...\n  Model saved to resnet152_mlp_biobart_best.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10 Summary:\n  Average Training Loss: 0.2638\n  Validation Loss:       0.3882\n  Validation loss did not improve. Patience: 1/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/10 Summary:\n  Average Training Loss: 0.2429\n  Validation Loss:       0.3917\n  Validation loss did not improve. Patience: 2/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/10 Summary:\n  Average Training Loss: 0.2231\n  Validation Loss:       0.3966\n  Validation loss did not improve. Patience: 3/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/10 Summary:\n  Average Training Loss: 0.1945\n  Validation Loss:       0.4076\n  Validation loss did not improve. Patience: 4/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/10 Summary:\n  Average Training Loss: 0.1820\n  Validation Loss:       0.4090\n  Validation loss did not improve. Patience: 5/5\n\nEarly stopping triggered after 10 epochs.\n\nTraining finished.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Evaluation function using BioBART\nimport torch\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\nimport numpy as np\n\n# Ensure NLTK resources are downloaded\nimport nltk\n# Download required NLTK resources\nprint(\"Downloading NLTK punkt tokenizer...\")\nnltk.download('punkt', quiet=True)\nprint(\"Downloading NLTK wordnet...\")\nnltk.download('wordnet', quiet=True)\n\ndef evaluate_model(mlp, biobart_model, test_loader, biobart_tokenizer, device):\n    \"\"\"Evaluates the model on the test set using BLEU, METEOR, and ROUGE metrics.\"\"\"\n    \n    # Ensure models are in evaluation mode\n    mlp.eval()\n    biobart_model.eval()\n    \n    generated_texts = []\n    reference_texts = []\n    \n    print(\"\\nStarting evaluation on the test set...\")\n    # Disable gradient calculations\n    with torch.no_grad():\n        progress_bar = tqdm(test_loader, desc='Evaluating', leave=False)\n        for input_batch, label_batch in progress_bar:\n            # Move data to the evaluation device\n            input_batch = input_batch.to(device)\n            # label_batch remains on CPU for decoding references easily\n            \n            # --- Generate Predictions --- \n            # 1. Get image embedding from MLP\n            embedded_input = mlp(input_batch) # Shape: (batch_size, hidden_size)\n            # Prepare for generate: needs shape (batch_size, seq_len, hidden_size)\n            embedded_input_gen = embedded_input.unsqueeze(1) # Shape: (batch_size, 1, hidden_size)\n            \n            try:\n                # 2. Generate text using BioBART\n                outputs = biobart_model.generate(\n                    inputs_embeds=embedded_input_gen,\n                    max_length=153, # Max length for generated report (adjust if needed)\n                    num_beams=4,\n                    early_stopping=True,\n                    pad_token_id=biobart_tokenizer.pad_token_id,\n                    eos_token_id=biobart_tokenizer.eos_token_id,\n                    # Use model's default decoder_start_token_id\n                )\n                \n                # --- Decode and Store --- \n                # Decode generated sequences\n                batch_generated_texts = biobart_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n                # Decode reference sequences (original labels)\n                batch_reference_texts = biobart_tokenizer.batch_decode(label_batch, skip_special_tokens=True)\n                \n                # Clean up any extra spaces and store\n                for gen_text, ref_text in zip(batch_generated_texts, batch_reference_texts):\n                    generated_texts.append(gen_text.strip())\n                    reference_texts.append(ref_text.strip())\n            \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    print(f\"\\nCUDA OOM during evaluation. Skipping batch.\")\n                    torch.cuda.empty_cache()\n                    continue\n                else:\n                    print(f\"Runtime Error during evaluation: {str(e)}\")\n                    raise e # Re-raise other errors\n            \n            # Clear GPU memory periodically if needed\n            if device == torch.device(\"cuda\") and progress_bar.n % 20 == 0: # Every 20 batches\n                 torch.cuda.empty_cache()\n        \n        progress_bar.close()\n\n    # --- Calculate Metrics --- \n    if not generated_texts or not reference_texts:\n        print(\"Evaluation failed: No texts were generated or references found.\")\n        return\n        \n    # 1. BLEU Scores\n    print(\"\\nCalculating BLEU scores...\")\n    # Tokenize texts for BLEU calculation\n    generated_tokens = [word_tokenize(text.lower()) for text in generated_texts]\n    # Reference tokens need to be a list of lists for corpus_bleu\n    reference_tokens = [[word_tokenize(text.lower())] for text in reference_texts]\n    \n    # Use smoothing function for robustness, especially for shorter texts or smaller test sets\n    smoothing = SmoothingFunction().method1 \n    bleu1 = corpus_bleu(reference_tokens, generated_tokens, weights=(1.0, 0, 0, 0), smoothing_function=smoothing)\n    bleu2 = corpus_bleu(reference_tokens, generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n    bleu3 = corpus_bleu(reference_tokens, generated_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n    bleu4 = corpus_bleu(reference_tokens, generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n    \n    print(f\"  BLEU-1: {bleu1:.4f}\")\n    print(f\"  BLEU-2: {bleu2:.4f}\") \n    print(f\"  BLEU-3: {bleu3:.4f}\")\n    print(f\"  BLEU-4: {bleu4:.4f}\")\n    \n    # 2. METEOR Score\n    print(\"\\nCalculating METEOR score...\")\n    # NLTK's meteor_score expects tokenized references and hypotheses\n    meteor_scores = [meteor_score([ref[0]], gen) for ref, gen in zip(reference_tokens, generated_tokens)]\n    avg_meteor = np.mean(meteor_scores) # Use numpy for mean calculation\n    print(f\"  METEOR: {avg_meteor:.4f}\")\n    \n    # 3. ROUGE Scores\n    print(\"\\nCalculating ROUGE scores...\")\n    rouge1_scores = []\n    rouge2_scores = []\n    rouge3_scores = []\n    rouge4_scores = []\n    rougeL_scores = []\n    \n    for ref, gen in zip(reference_tokens, generated_tokens):\n        # Calculate ROUGE-1 (unigram overlap)\n        rouge1_scores.append(calculate_rouge_n(ref[0], gen, n=1))\n        \n        # Calculate ROUGE-2 (bigram overlap)\n        rouge2_scores.append(calculate_rouge_n(ref[0], gen, n=2))\n        \n        # Calculate ROUGE-3 (trigram overlap)\n        rouge3_scores.append(calculate_rouge_n(ref[0], gen, n=3))\n        \n        # Calculate ROUGE-4 (4-gram overlap)\n        rouge4_scores.append(calculate_rouge_n(ref[0], gen, n=4))\n        \n        # Calculate ROUGE-L (longest common subsequence)\n        rougeL_scores.append(calculate_rouge_l(ref[0], gen))\n    \n    # Calculate averages\n    avg_rouge1 = np.mean(rouge1_scores)\n    avg_rouge2 = np.mean(rouge2_scores)\n    avg_rouge3 = np.mean(rouge3_scores)\n    avg_rouge4 = np.mean(rouge4_scores)\n    avg_rougeL = np.mean(rougeL_scores)\n    \n    # Print ROUGE scores\n    print(f\"  ROUGE-1: {avg_rouge1:.4f}\")\n    print(f\"  ROUGE-2: {avg_rouge2:.4f}\")\n    print(f\"  ROUGE-3: {avg_rouge3:.4f}\")\n    print(f\"  ROUGE-4: {avg_rouge4:.4f}\")\n    print(f\"  ROUGE-L: {avg_rougeL:.4f}\")\n    \n    # --- Print Example Generations --- \n    print(\"\\nExample Generations (Test Set):\")\n    num_examples = min(5, len(generated_texts)) # Show up to 5 examples\n    for i in range(num_examples):\n        print(f\"\\n--- Example {i+1} ---\")\n        print(f\"  Generated: {generated_texts[i]}\")\n        print(f\"  Reference: {reference_texts[i]}\")\n        print(\"-----------------\")\n\n# Helper function for ROUGE-N calculation\ndef calculate_rouge_n(ref_tokens, gen_tokens, n=1):\n    \"\"\"Calculates ROUGE-N F1 score based on n-gram overlap.\"\"\"\n    if len(ref_tokens) == 0 or len(gen_tokens) == 0:\n        return 0.0\n    \n    # Create n-grams\n    def get_ngrams(tokens, n):\n        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n    \n    ref_ngrams = get_ngrams(ref_tokens, n)\n    gen_ngrams = get_ngrams(gen_tokens, n)\n    \n    if not ref_ngrams or not gen_ngrams:\n        return 0.0\n    \n    # Count matching n-grams\n    ref_ngram_counts = {}\n    for ngram in ref_ngrams:\n        ref_ngram_counts[ngram] = ref_ngram_counts.get(ngram, 0) + 1\n    \n    gen_ngram_counts = {}\n    for ngram in gen_ngrams:\n        gen_ngram_counts[ngram] = gen_ngram_counts.get(ngram, 0) + 1\n    \n    # Count overlapping n-grams\n    overlap_count = 0\n    for ngram, count in gen_ngram_counts.items():\n        overlap_count += min(count, ref_ngram_counts.get(ngram, 0))\n    \n    # Calculate precision and recall\n    precision = overlap_count / len(gen_ngrams) if gen_ngrams else 0.0\n    recall = overlap_count / len(ref_ngrams) if ref_ngrams else 0.0\n    \n    # Calculate F1 score\n    if precision + recall == 0:\n        return 0.0\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n        \n# Helper function for ROUGE-L calculation using Longest Common Subsequence (LCS)\ndef calculate_rouge_l(ref_tokens, gen_tokens):\n    \"\"\"Calculates ROUGE-L F1 score based on LCS.\"\"\"\n    # Find the length of the Longest Common Subsequence\n    m, n = len(ref_tokens), len(gen_tokens)\n    if m == 0 or n == 0: return 0.0\n    # Initialize DP table (LCS lengths)\n    L = [[0] * (n + 1) for _ in range(m + 1)]\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if ref_tokens[i-1] == gen_tokens[j-1]:\n                L[i][j] = L[i-1][j-1] + 1\n            else:\n                L[i][j] = max(L[i-1][j], L[i][j-1])\n    lcs_len = L[m][n]\n    \n    # Calculate Precision, Recall, and F1 for ROUGE-L\n    recall = lcs_len / m if m > 0 else 0.0\n    precision = lcs_len / n if n > 0 else 0.0\n    \n    if recall + precision == 0:\n        f1 = 0.0\n    else:\n        f1 = (2 * recall * precision) / (recall + precision)\n    return f1\n\n# --- Run Evaluation --- \n# Ensure the test_loader is defined and models are loaded (potentially best checkpoint)\nif 'test_loader' in locals() and test_loader is not None:\n    # Optional: Load the best model checkpoint before evaluating\n    if os.path.exists(checkpoint_path):\n        print(f\"\\nLoading best model from {checkpoint_path} for final evaluation...\")\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        mlp.load_state_dict(checkpoint['mlp_state_dict'])\n        biobart_model.load_state_dict(checkpoint['biobart_state_dict'])\n        print(f\"Best model (Epoch {checkpoint['epoch']+1}) loaded.\")\n    else:\n        print(f\"Warning: Checkpoint {checkpoint_path} not found. Evaluating with current model state.\")\n        \n    evaluate_model(mlp, biobart_model, test_loader, biobart_tokenizer, device)\nelse:\n    print(\"\\nError: Test loader not defined. Cannot run evaluation.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T11:13:13.638117Z","iopub.execute_input":"2025-04-22T11:13:13.638397Z","iopub.status.idle":"2025-04-22T11:13:34.414880Z","shell.execute_reply.started":"2025-04-22T11:13:13.638378Z","shell.execute_reply":"2025-04-22T11:13:34.414121Z"}},"outputs":[{"name":"stdout","text":"Downloading NLTK punkt tokenizer...\nDownloading NLTK wordnet...\n\nLoading best model from resnet152_mlp_biobart_best.pt for final evaluation...\nBest model (Epoch 5) loaded.\n\nStarting evaluation on the test set...\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nCalculating BLEU scores...\n  BLEU-1: 0.3356\n  BLEU-2: 0.2033\n  BLEU-3: 0.1231\n  BLEU-4: 0.0772\n\nCalculating METEOR score...\n  METEOR: 0.3145\n\nCalculating ROUGE scores...\n  ROUGE-1: 0.4431\n  ROUGE-2: 0.1626\n  ROUGE-3: 0.0576\n  ROUGE-4: 0.0270\n  ROUGE-L: 0.3623\n\nExample Generations (Test Set):\n\n--- Example 1 ---\n  Generated: startseq the cardiomediastinal silhouette normal size and contour . no focal consolidation pneumothora large pleural effusion . negative for acute bone abnormality . endseq\n  Reference: startseq there calcified granuloma left midlung . there round density within the anterior segment the right upper lobe . there are prominent interstitial opacities which may represent changes associated with fibrosis . heart size normal . no pneumothora . anterior segment upper lobe rounded focal density . could lung nodule . endseq\n-----------------\n\n--- Example 2 ---\n  Generated: startseq the cardiomediastinal silhouette normal size and contour . no focal consolidation pneumothora large pleural effusion . negative for acute bone abnormality . endseq\n  Reference: startseq heart size normal . the lungs are clear . no pneumothora pleural effusion . endseq\n-----------------\n\n--- Example 3 ---\n  Generated: startseq heart size normal . lungs are clear . are normal . no pneumonia effusions edema pneumothora adenopathy nodules masses . endseq\n  Reference: startseq the heart normal size . the mediastinum unremarkable . the lungs are clear . endseq\n-----------------\n\n--- Example 4 ---\n  Generated: startseq the cardiomediastinal silhouette normal size and contour . no focal consolidation pneumothora large pleural effusion . negative for acute bone abnormality . endseq\n  Reference: startseq there mild hyperinflation . there no focal consolidation . there no pneumothora large pleural effusion . the cardiomediastinal contours are grossly unremarkable . the heart size within normal limits . cardiac generator overlies the left upper thora with tips overlying the right atrium and ventricles . endseq\n-----------------\n\n--- Example 5 ---\n  Generated: startseq the cardiomediastinal silhouette normal size and contour . no focal consolidation pneumothora large pleural effusion . negative for acute bone abnormality . endseq\n  Reference: startseq low lung volumes . normal heart size . no pneumothora . no large effusion . no focal infiltrate . endseq\n-----------------\n","output_type":"stream"}],"execution_count":19}]}